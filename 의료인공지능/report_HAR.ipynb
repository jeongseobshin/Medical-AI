{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"report_HAR.ipynb","private_outputs":true,"provenance":[],"mount_file_id":"1-ddun1f54c52HmKyQ79536BLe12c1Rt-","authorship_tag":"ABX9TyPq/t0Z4fzxJDlIOP/50YJ4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"-4pjcZkv2xMl"},"source":["#!/usr/bin/env python3\n","# -*- coding: utf-8 -*-\n","\"\"\"\n","Created on Wed Sep 27 14:34:20 2017\n","This is a small project for CNN in KERAS.\n","This file creates, trains and save a convolutional neural network for\n","Human Acitivity Recognition. The data we used for this file is released and provided by\n","Wireless Sensor Data Mining (WISDM) lab and can be found on this link.\n","http://www.cis.fordham.edu/wisdm/dataset.php  \n","Feel free to use this code and site this repositry if you use it for your reports or project.\n","@author: Muhammad Shahnawaz\n","\"\"\"\n","# importing libraries and dependecies \n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from scipy import stats\n","from keras.models import Sequential\n","from keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout\n","#from keras import backend as K\n","from keras import optimizers\n","\n","# setting up a random seed for reproducibility\n","random_seed = 611\n","np.random.seed(random_seed)\n","\n","# matplotlib inline\n","plt.style.use('ggplot')\n","\n","# defining function for loading the dataset\n","def readData(filePath):\n","# attributes of the dataset\n","    columnNames = ['user_id','activity','timestamp','x-axis','y-axis','z-axis']\n","    data = pd.read_csv(filePath,header = None, names=columnNames,na_values=';')\n","    return data\n","\n","# defining a function for feature normalization\n","# (feature - mean)/stdiv\n","def featureNormalize(dataset):\n","    mu = np.mean(dataset,axis=0) #mean 평균값\n","    sigma = np.std(dataset,axis=0) #std 표준편차\n","    return (dataset-mu)/sigma\n","\n","# defining the function to plot a single axis data\n","def plotAxis(axis,x,y,title):\n","    axis.plot(x,y)\n","    axis.set_title(title)\n","    axis.xaxis.set_visible(False)\n","    axis.set_ylim([min(y)-np.std(y),max(y)+np.std(y)])\n","    axis.set_xlim([min(x),max(x)])\n","    axis.grid(True)\n","\n","# defining a function to plot the data for a given activity\n","def plotActivity(activity,data):\n","    fig,(ax0,ax1,ax2) = plt.subplots(nrows=3, figsize=(15,10),sharex=True)\n","    plotAxis(ax0,data['timestamp'],data['x-axis'],'x-axis')\n","    plotAxis(ax1,data['timestamp'],data['y-axis'],'y-axis')\n","    plotAxis(ax2,data['timestamp'],data['z-axis'],'z-axis')\n","    plt.subplots_adjust(hspace=0.2)\n","    fig.suptitle(activity)\n","    plt.subplots_adjust(top=0.9)\n","    plt.show()\n","\n","# defining a window function for segmentation purposes\n","# data.count() 값이 약 1,000,000 이상으로 너무 크므로 200,000으로 두고 테스트\n","def windows(data, size):\n","    start = 0\n","    while start < data.count():\n","        yield int(start), int(start + size)\n","        start += (size/2)\n","\n","# segmenting the time series\n","def segment_signal(data, window_size = 90):\n","    segments = np.empty((0,window_size,3))\n","    labels= np.empty((0))\n","\n","    i = 0\n","    for (start, end) in windows(data['timestamp'],window_size):\n","        x = data['x-axis'][start:end]\n","        y = data['y-axis'][start:end]\n","        z = data['z-axis'][start:end]\n","        if(len(data['timestamp'][start:end])==window_size):\n","            segments = np.vstack([segments, np.dstack([x,y,z])])\n","            labels = np.append(labels,stats.mode(data['activity'][start:end])[0][0])\n","#print (i)\n","            i = i+1\n","    return segments, labels\n","\n","# main program #\n","# columnNames = ['user_id','activity','timestamp','x-axis','y-axis','z-axis']\n","\n","dataset = readData('/content/drive/MyDrive/python/data/actitracker_raw.txt')\n","for i in range(5):\n","    print (dataset['activity'][i], dataset['timestamp'][i], dataset['x-axis'][i])\n","\n","# plotting a subset of the data to visualize\n","for activity in np.unique(dataset['activity']):\n","    subset = dataset[dataset['activity']==activity][:180]\n","    plotActivity(activity,subset)\n","\n","# segmenting the signal in overlapping windows of 90 samples with 50% overlap\n","segments, labels = segment_signal(dataset) \n","\n","# categorically defining the classes of the activities\n","labels = np.asarray(pd.get_dummies(labels), dtype=np.int8)\n","\n","# defining parameters for the input and network layers\n","# we are treating each segment or chunk as a 2D image (90 X 3)\n","\n","print (segments.shape)\n","\n","h = segments.shape[1]\n","w = segments.shape[2]\n","numChannels = 1\n","numFilters = 128 # number of filters in Conv2D layer\n","numNueronsFCL1 = 128 # number of filters in fully connected layers\n","numNueronsFCL2 = 128\n","\n","trainRatio = 0.8  # split ratio for test and validation\n","Epochs = 10 # number of epochs\n","batchSize = 10 # batchsize\n","numClasses = labels.shape[1] # number of total clases\n","dropOutRatio = 0.2  # dropout ratio for dropout layer\n","\n","# reshaping the data for network input\n","reshapedSegments = segments.reshape(segments.shape[0], h, w, 1)\n","\n","# splitting in training and testing data\n","trainSplit = np.random.rand(len(reshapedSegments)) < trainRatio\n","trainX = reshapedSegments[trainSplit]\n","testX = reshapedSegments[~trainSplit]\n","trainX = np.nan_to_num(trainX)\n","testX = np.nan_to_num(testX)\n","trainY = labels[trainSplit]\n","testY = labels[~trainSplit]\n","\n","def cnnModel():\n","\n","    model = Sequential()\n","\n","    model.add(Conv2D(numFilters, (2,2), input_shape=(h, w,1), activation='relu'))\n","    model.add(MaxPooling2D(pool_size=(2,2),padding='valid'))\n","    model.add(Dropout(dropOutRatio))\n","\n","    model.add(Flatten())\n","    model.add(Dense(numNueronsFCL1, activation='relu'))\n","    model.add(Dense(numNueronsFCL2, activation='relu'))\n","    model.add(Dense(numClasses, activation='softmax'))\n","\n","    adam = optimizers.Adam(lr = 0.001, decay=1e-6)\n","    model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n","    return model\n","\n","\n","model = cnnModel()\n","model.summary()\n","\n","model.fit(trainX, trainY, validation_split=1-trainRatio, epochs=10, batch_size=32, verbose=2)\n","\n","score = model.evaluate(testX,testY,verbose=2)\n","print('Correct Classification : %.2f%%' %(score[1]*100))\n","\n","model.save('model.h5')\n","np.save('groundTruth.npy',testY)\n","np.save('testData.npy',testX)"],"execution_count":null,"outputs":[]}]}
